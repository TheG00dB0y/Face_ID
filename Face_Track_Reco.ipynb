{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face_Track_Reco.ipynb",
      "provenance": [],
      "mount_file_id": "1l7V4XMglNC6GczDXRI6k6RijB_O8fCvC",
      "authorship_tag": "ABX9TyNdF+ngfadFcXrwY/DFr6VS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheG00dB0y/Face_ID/blob/master/Face_Track_Reco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmje9dW7Anqv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9dc7b2c-6c69-43da-a9e4-6bb02c14a9c8"
      },
      "source": [
        "!rm \"/content/Face_ID/facepics\" -r\n",
        "!rm \"/content/Face_ID/outputs\" -r"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/Face_ID/facepics': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_0-7y6yf-ou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3161121-b487-43eb-a3ec-534e316318bd"
      },
      "source": [
        "!git clone https://github.com/TheG00dB0y/Face_ID"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Face_ID' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D21MWF_YgGbd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7ea35559-2577-4fc5-e3a5-3f4ef22c6e10"
      },
      "source": [
        "%cd Face_ID/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Face_ID\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGio41WXHvod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/drive/My Drive/yoloface/cfg\" \"cfg\" -r\n",
        "!cp \"/content/drive/My Drive/yoloface/model-weights\" \"model-weights\" -r\n",
        "!cp \"/content/drive/My Drive/yoloface/samples\" \"samples\" -r\n",
        "!cp \"/content/drive/My Drive/yoloface/utils.py\" \"utils.py\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtUUHGKlX3rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir \"facepics\"\n",
        "!mkdir \"outputs\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mwe3CDayhAz2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "13afb368-fe85-4bd3-fc88-3debf0c3b583"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "#face_rec\n",
        "import tensorflow as tf\n",
        "from fr_utils import *\n",
        "from inception_blocks_v2 import *\n",
        "from utils import *"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozAdxenmaiYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    box1 -- first box, list object with coordinates (box1_x1, box1_y1, box1_x2, box_1_y2)\n",
        "    box2 -- second box, list object with coordinates (box2_x1, box2_y1, box2_x2, box2_y2)\n",
        "    \"\"\"\n",
        "    # Assign variable names to coordinates for clarity\n",
        "    (box1_x1, box1_y1, box1_x2, box1_y2) = box1\n",
        "    (box2_x1, box2_y1, box2_x2, box2_y2) = box2\n",
        "    \n",
        "    # Calculate the (yi1, xi1, yi2, xi2) coordinates of the intersection of box1 and box2. Calculate its Area.\n",
        "    ### START CODE HERE ### (≈ 7 lines)\n",
        "    xi1 = max(box1_x1,box2_x1)\n",
        "    yi1 = max(box1_y1,box2_y1)\n",
        "    xi2 = min(box1_x2,box2_x2)\n",
        "    yi2 = min(box1_y2,box2_y2)\n",
        "    inter_width = xi2-xi1\n",
        "    inter_height = yi2-yi1\n",
        "    inter_area = max(inter_width,0) * max(inter_height,0)\n",
        "    ### END CODE HERE ###    \n",
        "\n",
        "    # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)\n",
        "    ### START CODE HERE ### (≈ 3 lines)\n",
        "    box1_area = (box1_x2-box1_x1) * (box1_y2-box1_y1)\n",
        "    box2_area = (box2_x2-box2_x1) * (box2_y2-box2_y1)\n",
        "    union_area = (box1_area + box2_area) - inter_area\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # compute the IoU\n",
        "    ### START CODE HERE ### (≈ 1 line)\n",
        "    iou = inter_area / union_area\n",
        "    ### END CODE HERE ###\n",
        "    return iou\n",
        "class Sort(object):\n",
        "  def __init__(self, max_age=1, iou_thresh =0.4):\n",
        "    \"\"\"\n",
        "    Sets key parameters for SORT\n",
        "    \"\"\"\n",
        "    #Age denotes the expiring of a track\n",
        "    self.max_age = max_age\n",
        "    self.iou_thresh = iou_thresh\n",
        "    self.p_track = []\n",
        "    self.frame_count = 0\n",
        "    self.id = 0\n",
        "    \n",
        "  def update(self, dets=np.empty((0, 5))):\n",
        "    # print(dets)\n",
        "    self.frame_count += 1\n",
        "    # self.p_track[:]['age'] += 1\n",
        "    updated_trk = []\n",
        "    # if self.id==0 and dets.size != 0:\n",
        "    if not self.p_track and dets.size != 0:\n",
        "      #Initialise first detections as Previous Trackers & if not first add with previous id(When p_track becomes empty)\n",
        "      self.p_track = [{'bbox':list(det),'id':k + self.id,'age':0,'frame_count':1} for k,det in enumerate(dets,1)]\n",
        "      self.id = self.p_track[-1]['id']\n",
        "    elif self.p_track and dets.size != 0:\n",
        "      # print(\"####\")\n",
        "      # print(self.p_track)\n",
        "      new_p = self.p_track\n",
        "      for det in dets:\n",
        "        try:\n",
        "          best_match_trk = max(new_p,key=lambda x:iou(det,x['bbox']))\n",
        "        except ValueError:\n",
        "          print(\"################################### VALUE ERROR ####################################################\\n\"+\"det: \",end=\" \")\n",
        "          print(det)\n",
        "          print(\"prev trk: \",end=\" \")\n",
        "          print(self.p_track)\n",
        "          continue\n",
        "        # print(iou(det,best_match_trk['bbox']))\n",
        "        if iou(det,best_match_trk['bbox']) >= self.iou_thresh :\n",
        "          best_match_trk['bbox'] = det\n",
        "          best_match_trk['frame_count'] += 1\n",
        "          # print(best_match_trk)\n",
        "          #reset age\n",
        "          best_match_trk['age'] = 0\n",
        "          updated_trk.append(best_match_trk)\n",
        "        else:\n",
        "          #New non matched detection is assigned to new track\n",
        "          self.id += 1\n",
        "          new_trk = {'bbox':det,'id':self.id,'age':0,'frame_count':1}\n",
        "          updated_trk.append(new_trk)\n",
        "          if best_match_trk['age'] <= self.max_age:\n",
        "            best_match_trk['age'] += 1\n",
        "            updated_trk.append(best_match_trk)\n",
        "        #removing the used best_match_trk from p_track\n",
        "        # del dets[dets.index(best_match)]\n",
        "        # print(self.p_track[self.p_track.index(best_match_trk)])\n",
        "        # print(\"before:\")\n",
        "        # print(self.p_track)\n",
        "        # print(best_match_trk)\n",
        "        # print(self.p_track.index(best_match_trk))\n",
        "        # del self.p_track[self.p_track.index(best_match_trk)]\n",
        "        # self.p_track.remove(best_match_trk)\n",
        "        self.p_track = list(filter(lambda x: x['id'] != best_match_trk['id'] and x['age'] < self.max_age, self.p_track)) \n",
        "        # print(\"after:\")\n",
        "        # print(self.p_track)        \n",
        "      #increase age of previous tracks\n",
        "      for trk in self.p_track:\n",
        "        trk['age'] += 1\n",
        "      self.p_track = self.p_track + updated_trk\n",
        "    elif self.p_track and dets.size == 0:\n",
        "      new_p = []\n",
        "      for trk in self.p_track:\n",
        "        trk['age'] += 1\n",
        "        if(trk['age'] < self.max_age):\n",
        "          new_p.append(trk)\n",
        "      self.p_track = new_p\n",
        "      return []\n",
        "        \n",
        "      \n",
        "    # print(\"#######\",end = \" \")\n",
        "    # print(self.p_track,end=\"#######\")\n",
        "    return self.p_track\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqf_aiPbb0rl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "69034c1d-91e9-423b-a10f-04df2f8e132b"
      },
      "source": [
        "model_cfg = './cfg/yolov3-face.cfg'\n",
        "model_weights = './model-weights/yolov3-wider_16000.weights'\n",
        "# image = './samples/outside_000001.jpg'\n",
        "video = './samples/sss.mp4'\n",
        "# output_dir = '/content/drive/My Drive/yoloface/outputs/'\n",
        "output_dir = './outputs/'\n",
        "\n",
        "IOU_THRESH = 0.35\n",
        "MAX_AGE = 3\n",
        "#Trackers(Here faces) with given frame count value is chosen for age and gender processing\n",
        "FRAME_COUNT_THRESH = 6\n",
        "# DETECT_INTERVAL = 2\n",
        "FACE_HEIGHT = 50\n",
        "FACE_WIDTH = 50\n",
        "# print the arguments\n",
        "print('----- info -----')\n",
        "print('[i] The config file: ',model_cfg)\n",
        "print('[i] The weights of model file: ',model_weights)\n",
        "# print('[i] Path to image file: ',image)\n",
        "print('[i] Path to video file: ',video)\n",
        "print('[i] IOU Threshold value: ',IOU_THRESH)\n",
        "print('[i] Max age value: ',MAX_AGE)\n",
        "print('[i] Frame count threshold value: ',FRAME_COUNT_THRESH)\n",
        "# print('[i] Detect interval: ',DETECT_INTERVAL)\n",
        "print('[i] Minimum face height required: ',FACE_HEIGHT)\n",
        "print('[i] Minimum face width required: ',FACE_WIDTH)\n",
        "\n",
        "\n",
        "#Read config net\n",
        "net = cv2.dnn.readNetFromDarknet(model_cfg,model_weights)\n",
        "print('###########################################################\\n')\n",
        "\n",
        "\n",
        "#VIDEO\n",
        "cap = cv2.VideoCapture(video)\n",
        "output_file = video[:-4].rsplit('/')[-1] + '_keras.avi'\n",
        "output_file = output_dir + output_file\n",
        "\n",
        "# cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
        "# cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
        "\n",
        "video_writer = cv2.VideoWriter(output_file,cv2.VideoWriter_fourcc(*'XVID'),cap.get(cv2.CAP_PROP_FPS), (\n",
        "                                          round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "                                          round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
        "\n",
        "\n",
        "# print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "# print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "print(cap.get(cv2.CAP_PROP_FPS))\n",
        "#create instance of SORT\n",
        "mot = Sort(MAX_AGE,IOU_THRESH)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- info -----\n",
            "[i] The config file:  ./cfg/yolov3-face.cfg\n",
            "[i] The weights of model file:  ./model-weights/yolov3-wider_16000.weights\n",
            "[i] Path to video file:  ./samples/sss.mp4\n",
            "[i] IOU Threshold value:  0.35\n",
            "[i] Max age value:  3\n",
            "[i] Frame count threshold value:  6\n",
            "[i] Minimum face height required:  50\n",
            "[i] Minimum face width required:  50\n",
            "###########################################################\n",
            "\n",
            "23.976023976023978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuaSWxPcijli",
        "colab_type": "text"
      },
      "source": [
        "Face Recognisation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUswB8ta4wLg",
        "colab_type": "text"
      },
      "source": [
        "**BUG**:\n",
        "**In line 506 I changed line**\n",
        "\n",
        "_LOCAL_DEVICES = tf.config.experimental_list_devices()\n",
        "\n",
        "to\n",
        "\n",
        "devices = tf.config.list_logical_devices()\n",
        "\n",
        "_LOCAL_DEVICES = [x.name for x in devices]\n",
        "\n",
        "**And restart runtime**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgHQ9rVgeC5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new model instance\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "FRmodel = faceRecoModel(input_shape=(3, 96, 96))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvU7STTqiMQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Restore the weights\n",
        "FRmodel.load_weights('./weight_file')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCZNTfE56nAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def img_encoding(img1, model):\n",
        "    img = img1[...,::-1]\n",
        "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
        "    x_train = np.array([img])\n",
        "    embedding = model.predict_on_batch(x_train)\n",
        "    return embedding"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxsNLP4fSFKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def who_is_it(img, database, model):\n",
        "        \n",
        "    encoding = img_encoding(img,model)\n",
        "    min_dist = 100\n",
        "    for (name, db_enc) in database.items():\n",
        "      # Compute L2 distance between the target \"encoding\" and the current db_enc from the database. (≈ 1 line)\n",
        "      dist = np.linalg.norm(db_enc-encoding)\n",
        "      # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n",
        "      if dist<min_dist:\n",
        "        min_dist = dist\n",
        "        identity = name\n",
        "\n",
        "    if min_dist > 0.7:\n",
        "        # print(\"Not in the database.\")\n",
        "        return ''\n",
        "    else:\n",
        "        # print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
        "        return identity\n",
        "        \n",
        "    return identity"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7FNd9c5t5nv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm /content/Face_ID/images/facepics -r"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_3a9wXZuDeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !unzip \"/content/file(1).zip\" -d /content/Face_ID/images"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exFXVeg5SFKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "database = {}\n",
        "database[\"Athul\"] = img_to_encoding(\"/content/Face_ID/images/content/age-gender-estimation/facepics/1.jpg\", FRmodel)\n",
        "database[\"Alveena\"] = img_to_encoding(\"/content/Face_ID/images/content/age-gender-estimation/facepics/2.jpg\", FRmodel)\n",
        "database[\"Ajay\"] = img_to_encoding(\"/content/Face_ID/images/content/age-gender-estimation/facepics/3.jpg\", FRmodel)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yijacsyK2XmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# who_is_it(\"/content/Face_ID/images/content/age-gender-estimation/facepics/1.jpg\", database, FRmodel)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoF1N_Jn3tFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
        "               font_scale=1.0, thickness=3):\n",
        "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
        "    x, y = point\n",
        "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
        "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9ogZTM86EmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "margin = 0.4"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb7dGPz0T2G6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "108584a1-cae0-4e4f-8da2-ffdc3b0a0fa0"
      },
      "source": [
        "#Frame counter for detect interval\n",
        "# c=0\n",
        "while True:\n",
        "  \n",
        "  has_frame, frame = cap.read()\n",
        "  if not has_frame:\n",
        "      print('[i] ==> Done processing!!!')\n",
        "      #print('[i] ==> Output file is stored at', os.path.join(args.output_dir, output_file))\n",
        "      break\n",
        "  # frame = cv2.resize(frame, (0, 0), fx=0.7, fy=0.7)\n",
        "  # Resize, Convert BGR to HSV\n",
        "  # if ((IMG_HEIGHT, IMG_WIDTH) != frame.shape[0:2]):\n",
        "  #     frame = cv2.resize(frame, dsize=(IMG_WIDTH, IMG_HEIGHT), fx=0, fy=0)\n",
        "  # else:\n",
        "  #     frame = frame\n",
        "  # if(c % DETECT_INTERVAL==0):\n",
        "  new_frame = frame.copy()\n",
        "  img_h, img_w, _ = np.shape(frame)\n",
        "\n",
        "  blob = cv2.dnn.blobFromImage(frame, 1 / 255, (IMG_WIDTH, IMG_HEIGHT),[0, 0, 0], 1, crop=False)\n",
        "  net.setInput(blob)\n",
        "  # Runs the forward pass to get output of the output layers\n",
        "  outs = net.forward(get_outputs_names(net))\n",
        "  # Remove the bounding boxes with low confidence\n",
        "  faces = post_process(frame, outs,CONF_THRESHOLD, NMS_THRESHOLD)\n",
        "  faces = np.array(faces) \n",
        "  \n",
        "  # faces_resized = []\n",
        "\n",
        "  if(faces.shape[0]>0):\n",
        "    final_faces = faces[:,:4]\n",
        "    \n",
        "  else:\n",
        "    final_faces = faces\n",
        "  trackers = mot.update(final_faces)\n",
        "  # print(\"Final faces:\",end=\" \")\n",
        "  # print(final_faces)\n",
        "  # print(\"$$$$$$$$$$$$$ \") \n",
        "  # print(trackers)\n",
        "  # print(\"&&&&&&&&&&&&&&&&&&\")\n",
        "  print(\"#\",end=\"\")\n",
        "  # faces_resized = np.empty((len(trackers), img_size, img_size, 3))\n",
        "\n",
        "  for i,trk in enumerate(trackers):\n",
        "    box = np.array(trk['bbox'])\n",
        "    id = int(trk['id'])\n",
        "    frame_count = trk['frame_count']\n",
        "    d = box.astype(np.int32)\n",
        "    (startX, startY, endX, endY) = (d[0],d[1],d[2],d[3])\n",
        "    face_h = endY - startY\n",
        "    face_w = endX - startX\n",
        "\n",
        "    x1, y1, x2, y2, w, h = d[0], d[1], d[2] + 1, d[3] + 1, face_w, face_h\n",
        "    xw1 = max(int(x1 - margin * w), 0)\n",
        "    yw1 = max(int(y1 - margin * h), 0)\n",
        "    xw2 = min(int(x2 + margin * w), img_w - 1)\n",
        "    yw2 = min(int(y2 + margin * h), img_h - 1)\n",
        "    # cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
        "    cv2.rectangle(frame, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\n",
        "    # faces[i, :, :, :] = cv2.resize(img[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # if(frame_count==FRAME_COUNT_THRESH and (face_h > FACE_HEIGHT and face_w > FACE_WIDTH)):\n",
        "    # crop_face = new_frame[startY:endY, startX:endX]\n",
        "    # crop_face_shape = new_frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
        "    crop_face = cv2.resize(new_frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (96, 96))\n",
        "    # crop_face = cv2.resize( new_frame[startY:endY, startX:endX], (96, 96))\n",
        "    print(crop_face.shape)\n",
        "    # faces_resized[i,:,:,:] = crop_face\n",
        "\n",
        "    if(frame_count == FRAME_COUNT_THRESH):  \n",
        "      try:\n",
        "        cv2.imwrite(\"{0}/{1}.jpg\".format(\"facepics\",id),crop_face)\n",
        "        # cv2_imshow(crop_face)\n",
        "      except:\n",
        "        print(\"#########################Image error####################\")\n",
        "        continue\n",
        "    identity = who_is_it(crop_face, database, FRmodel)    \n",
        "    # print(identity)\n",
        "    label = \"{}, {}\".format(id,identity)\n",
        "    draw_label(frame, (d[0], d[1]), label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # cv2_imshow(frame)\n",
        "  video_writer.write(frame.astype(np.uint8))\n",
        "  # c+=1\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-2498a3b5d1b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# Runs the forward pass to get output of the output layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_outputs_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0;31m# Remove the bounding boxes with low confidence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCONF_THRESHOLD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNMS_THRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhXshRFJgo0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap.release()\n",
        "video_writer.release()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV77X3SR9rGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}